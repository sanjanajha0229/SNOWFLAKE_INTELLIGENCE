{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "thdvn4s2w6zw4mnrgdlo",
   "authorId": "4966030068471",
   "authorName": "SANJANAJHA",
   "authorEmail": "divyansh.Saxena@divyanshsnowflake.onmicrosoft.com",
   "sessionId": "81fa0567-643f-4b0a-8a8b-c03f2b29b3a9",
   "lastEditTime": 1768543367470
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell0"
   },
   "source": "\n# Snowpark Python bootstrap: pull file from stage, (Excel→CSV if needed), profile, chunk if large, and push CSV back to stage.\n# References: Python worksheets & Snowpark session/file APIs (docs)  [1](https://docs.snowflake.com/en/developer-guide/snowpark/python/python-worksheets)[2](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.FileOperation.put)\n\nimport os, io, time, json, logging\nfrom datetime import datetime\nimport pandas as pd\nfrom pandas.api.types import is_numeric_dtype, is_datetime64_any_dtype\nfrom snowflake.snowpark.context import get_active_session\n\n# ---- Config (edit if needed) ----\nDB = \"BCG\"\nSCHEMA = \"BCG_SCHEMA\"                  # space in name → keep quotes in SQL cells\nSTAGE = \"BCG_STAGE\"\nSRC_FILE = \"Data for POC project data 1 (1).csv\"  # current staged file name\nTARGET_CSV = \"data_for_poc_project_data_1.csv\"    # normalized CSV name in stage\nLOCAL_DIR = \"/tmp/pb_ingest\"\nos.makedirs(LOCAL_DIR, exist_ok=True)\n\nsession = get_active_session()\nlogging.getLogger().setLevel(logging.INFO)\n\ndef stage_url(db, schema, stage, file=None):\n    quoted_schema = f'\"{schema}\"'\n    base = f'@{db}.{quoted_schema}.{stage}'\n    return f\"{base}/{file}\" if file else base\n\n# ---- Download from stage ----\nt0 = time.time()\nget_res = session.file.get(stage_url(DB, SCHEMA, STAGE, SRC_FILE), LOCAL_DIR)\nlogging.info(f\"GET results: {get_res}\")\nlocal_path = os.path.join(LOCAL_DIR, SRC_FILE)\nassert os.path.exists(local_path), f\"Local file not found: {local_path}\"\n\n# ---- Read & convert (supports both .xlsx and .csv) ----\next = os.path.splitext(local_path)[1].lower()\nif ext in [\".xlsx\", \".xls\"]:\n    df = pd.read_excel(local_path, engine=\"openpyxl\")\n    # Write to normalized CSV path\n    csv_path = os.path.join(LOCAL_DIR, TARGET_CSV)\n    df.to_csv(csv_path, index=False)\n    logging.info(f\"Converted Excel → CSV: {csv_path}\")\nelse:\n    # Already CSV: load and re-save to normalized name to enforce consistent naming\n    df = pd.read_csv(local_path)\n    csv_path = os.path.join(LOCAL_DIR, TARGET_CSV)\n    df.to_csv(csv_path, index=False)\n    logging.info(f\"CSV normalized: {csv_path}\")\n\n# ---- Profile schema & emit data dictionary ----\nprofile = []\nn_rows, n_cols = df.shape\nfor col in df.columns:\n    series = df[col]\n    nulls = int(series.isna().sum())\n    null_pct = round((nulls / len(series)) * 100, 2) if len(series) else 0.0\n    card = int(series.nunique(dropna=True))\n    dtype = str(series.dtype)\n\n    min_val = max_val = None\n    samples = None\n    if is_numeric_dtype(series):\n        s = pd.to_numeric(series, errors=\"coerce\")\n        min_val = float(s.min()) if s.notna().any() else None\n        max_val = float(s.max()) if s.notna().any() else None\n    elif is_datetime64_any_dtype(series):\n        s = pd.to_datetime(series, errors=\"coerce\")\n        min_val = s.min().isoformat() if pd.notna(s.min()) else None\n        max_val = s.max().isoformat() if pd.notna(s.max()) else None\n    else:\n        s = series.astype(str)\n        lengths = s.where(series.notna(), None).dropna().str.len()\n        min_val = int(lengths.min()) if len(lengths) else None\n        max_val = int(lengths.max()) if len(lengths) else None\n        samples = list(series.dropna().astype(str).unique()[:5])\n\n    profile.append({\n        \"column\": col, \"dtype\": dtype, \"nulls\": nulls, \"null_pct\": null_pct,\n        \"cardinality\": card, \"min\": min_val, \"max\": max_val, \"samples\": samples\n    })\n\nsummary = {\"rows\": n_rows, \"columns\": n_cols, \"data_dictionary\": profile}\ndict_path = os.path.join(LOCAL_DIR, \"data_profile_summary.json\")\nwith open(dict_path, \"w\") as f:\n    json.dump(summary, f, indent=2)\nlogging.info(f\"Profile saved: {dict_path}\")\n\n# ---- Chunk large files (rare for 3k rows; included for robustness) ----\n# If file > ~100MB, split into chunks; else single file.\nchunks = [csv_path]\nsize_mb = os.path.getsize(csv_path) / (1024 * 1024)\nif size_mb > 100:\n    chunks = []\n    CHUNK_ROWS = 250_000\n    for i, start in enumerate(range(0, len(df), CHUNK_ROWS)):\n        part = df.iloc[start:start+CHUNK_ROWS]\n        part_path = os.path.join(LOCAL_DIR, f\"{os.path.splitext(TARGET_CSV)[0]}_part{i+1}.csv\")\n        part.to_csv(part_path, index=False)\n        chunks.append(part_path)\n    logging.info(f\"Chunked into {len(chunks)} part(s).\")\n\n# ---- Upload CSV back to stage (overwrite, auto-compress for transport) ----\nput_results = []\nfor path in chunks:\n    put_results += session.file.put(\n        path,\n        stage_url(DB, SCHEMA, STAGE),\n        auto_compress=True,\n        overwrite=True\n    )\nlogging.info(f\"PUT results: {put_results}\")  # session.file.put docs  [2](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.FileOperation.put)\n\n# ---- Timing & LIST sanity check ----\nelapsed = round(time.time() - t0, 2)\nprint(json.dumps({\n    \"rows\": n_rows, \"cols\": n_cols, \"uploaded_files\": [os.path.basename(p) for p in chunks],\n    \"elapsed_sec\": elapsed\n}, indent=2))\n\n# Optional: list stage files for confirmation (SQL LIST command is available separately in Cell 2).\n# Staging/PUT guidance  [3](https://docs.snowflake.cn/en/user-guide/data-load-local-file-system-stage)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a30fba9e-aed8-432e-beac-9e94c943a41f",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "\n\n    USE WAREHOUSE COMPUTE_WH;\n    USE DATABASE BCG;\n    USE SCHEMA \"BCG_SCHEMA\";\n\n    CREATE STAGE IF NOT EXISTS BCG_STAGE COMMENT = 'POC staging for billing/performance workbook';\n\n    CREATE OR REPLACE RESOURCE MONITOR RM_PB_WH WITH CREDIT_QUOTA = 150\n      TRIGGERS ON 80 PERCENT DO NOTIFY\n               ON 100 PERCENT DO SUSPEND;\n\n\n    CREATE TAG IF NOT EXISTS LINEAGE_SOURCE;\n    CREATE TAG IF NOT EXISTS DATA_CLASSIFICATION;\n    ALTER STAGE BCG_STAGE SET TAG LINEAGE_SOURCE = 'poc_upload', DATA_CLASSIFICATION = 'non_pii';\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2890742e-5da1-4440-861d-ef1c71f89563",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "\n# Upload normalized CSV(s) from local temp to @BCG.\"BCG SCHEMA\".BCG_STAGE and list files.\n# Snowpark FileOperation.put reference  [2](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.FileOperation.put)\n# Staging/LIST reference  [3](https://docs.snowflake.cn/en/user-guide/data-load-local-file-system-stage)\n\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\nDB, SCHEMA, STAGE = \"BCG\", \"BCG_SCHEMA\", \"BCG_STAGE\"\nquoted_schema = f'\"{SCHEMA}\"'\nstage_base = f'@{DB}.{quoted_schema}.{STAGE}'\n\n# Re-list the stage after uploads (Cell 0 already put the CSV).\nres = session.sql(f\"LIST {stage_base}\").collect()\nprint({\"stage\": stage_base, \"file_count\": len(res), \"files\": [r[1] for r in res]})\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d82ec9b5-ea07-4726-9d7c-e663f6d1157b",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "\n\n  CREATE OR REPLACE FILE FORMAT FF_CSV TYPE = CSV SKIP_HEADER = 1 FIELD_DELIMITER = ',' FIELD_OPTIONALLY_ENCLOSED_BY = '\\\"' NULL_IF = ('\\\\N','NULL','',' ') COMPRESSION = 'AUTO' COMMENT = 'CSV file format for POC billing';\n  CREATE OR REPLACE FILE FORMAT FF_JSON TYPE = JSON COMMENT = 'For extensibility (disabled by default)';\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7c290c8e-cfe4-4c19-a891-e7930c28f285",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "\nCREATE OR REPLACE TABLE T_BILLING_LANDING (\n  ProjectCode STRING,\n  ProjectName STRING,\n  ClientName STRING,\n  EmpID NUMBER(38,0),\n  EmpName STRING,\n  WeekStartDate NUMBER(38,0),   -- Excel serial → convert in curated view\n  WeekEndDate   NUMBER(38,0),   -- Excel serial → convert in curated view\n  CurrencyCode STRING,\n  HoursFilled NUMBER(18,2),\n  DailyHourRate NUMBER(18,2),\n  NetClientCharges NUMBER(18,2),\n  GrossClientCharges NUMBER(18,2),\n  PartnerEffortHours NUMBER(18,2),\n  PartnerCost NUMBER(18,2),\n  PartnerRevenue NUMBER(18,2),\n  TotalRevenue NUMBER(18,2),\n  Country STRING,\n  City STRING,\n  BillingStatus STRING,\n  InvoiceNumber STRING,\n  Department STRING,\n  Role STRING,\n  ProjectStartDate NUMBER(38,0), -- Excel serial\n  ProjectEndDate NUMBER(38,0),   -- Excel serial\n  \"Utilization%\" NUMBER(18,2),\n  \"DiscountApplied%\" NUMBER(18,2),\n  TaxAmount NUMBER(18,2),\n  BillingCycle STRING,\n  ProjectStatus STRING,\n  ClientIndustry STRING,\n  ManagerName STRING,\n  CostCenter STRING\n) COMMENT='Landing table: raw types from CSV (Excel serial dates), curated view will cast & validate';\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2423d5f8-0eb7-4e98-b0b0-258d05859d6d",
   "metadata": {
    "language": "python",
    "name": "cell3_1"
   },
   "outputs": [],
   "source": "\n# Cell 3.1 — Build & load weekly FX (landing first, fallback to stage CSV) — PATCHED\nfrom snowflake.snowpark.context import get_active_session\nimport pandas as pd\n\nsession = get_active_session()\n\n# Context\nsession.sql(\"USE WAREHOUSE COMPUTE_WH\").collect()\nsession.sql(\"USE DATABASE BCG\").collect()\nsession.sql(\"USE SCHEMA BCG_SCHEMA\").collect()\n\n# --- Ensure T_FX_RATES_WEEKLY exists, and if it exists with 3 cols, add the 4th ---\n# 1) Create the table if it doesn't exist at all (with the full 4-column schema)\nsession.sql(\"\"\"\nCREATE TABLE IF NOT EXISTS BCG.BCG_SCHEMA.T_FX_RATES_WEEKLY (\n  currency_code        STRING,\n  week_start_d         DATE,\n  rate_usd_per_curr    NUMBER(18,9),  -- USD per 1 local unit\n  rate_curr_per_usd    NUMBER(18,9)   -- Local per 1 USD (derived as 1 / rate_usd_per_curr)\n)\n\"\"\").collect()\n\n# 2) If the table exists but is still the old 3-column schema, add the 4th column\ncol_exists = session.sql(\"\"\"\nSELECT COUNT(*) AS c\nFROM BCG.INFORMATION_SCHEMA.COLUMNS\nWHERE TABLE_SCHEMA = 'BCG_SCHEMA'\n  AND TABLE_NAME   = 'T_FX_RATES_WEEKLY'\n  AND COLUMN_NAME  = 'RATE_CURR_PER_USD'\n\"\"\").collect()[0][0]\n\nif col_exists == 0:\n    session.sql(\"\"\"\n    ALTER TABLE BCG.BCG_SCHEMA.T_FX_RATES_WEEKLY\n    ADD COLUMN rate_curr_per_usd NUMBER(18,9)\n    \"\"\").collect()\n\n# Idempotent load: clear table to avoid duplicates on re-run\nsession.sql(\"TRUNCATE TABLE BCG.BCG_SCHEMA.T_FX_RATES_WEEKLY\").collect()\n\n# Determine source availability\nlanding_count = session.sql(\"SELECT COUNT(*) AS c FROM BCG.BCG_SCHEMA.T_BILLING_LANDING\").collect()\nuse_stage = (len(landing_count) == 0) or (landing_count[0][0] == 0)\n\n# Pull distinct (currency_code, week_start_d) from the best source\nif not use_stage:\n    qry = \"\"\"\n    SELECT DISTINCT\n      TRIM(CurrencyCode) AS currency_code,\n      COALESCE(\n        TRY_TO_DATE(WeekStartDate),\n        DATEADD('day', TRY_TO_NUMBER(WeekStartDate), '1899-12-30')\n      ) AS week_start_d\n    FROM BCG.BCG_SCHEMA.T_BILLING_LANDING\n    WHERE CurrencyCode IS NOT NULL\n      AND (TRY_TO_DATE(WeekStartDate) IS NOT NULL OR TRY_TO_NUMBER(WeekStartDate) IS NOT NULL)\n    \"\"\"\n    df = session.sql(qry).to_pandas()\nelse:\n    # Fallback: read from staged CSV using FF_CSV (created in Cell 4_1)\n    qry_stage = \"\"\"\n    SELECT DISTINCT\n      TRIM($8) AS currency_code,\n      COALESCE(\n        TRY_TO_DATE($6),\n        DATEADD('day', TRY_TO_NUMBER($6), '1899-12-30')\n      ) AS week_start_d\n    FROM @BCG.BCG_SCHEMA.BCG_STAGE/data_for_poc_project_data_1.csv\n    ( FILE_FORMAT => 'FF_CSV' )\n    WHERE $8 IS NOT NULL\n      AND (TRY_TO_DATE($6) IS NOT NULL OR TRY_TO_NUMBER($6) IS NOT NULL)\n    \"\"\"\n    df = session.sql(qry_stage).to_pandas()\n\n# Normalize and validate\ndf.columns = [c.lower() for c in df.columns]\nif df.empty:\n    raise ValueError(\"No (currency_code, week_start_d) found from landing or staged CSV. Confirm file name and column positions ($6=WeekStartDate, $8=CurrencyCode).\")\n\n# Synthetic FX map (edit/extend as needed)\nFX_MAP = {\n    \"USD\": 1.00,  # USD per USD\n    \"INR\": 0.012, # USD per INR\n    \"EUR\": 1.10,\n    \"GBP\": 1.27,\n    \"AUD\": 0.68,\n    \"CAD\": 0.74,\n    \"SGD\": 0.75,\n    \"AED\": 0.27,\n    \"CHF\": 1.25\n}\n\ndef usd_per_local(curr: str):\n    curr = (curr or \"\").upper().strip()\n    return FX_MAP.get(curr, None)\n\ndf[\"currency_code\"] = df[\"currency_code\"].str.upper().str.strip()\ndf[\"rate_usd_per_curr\"] = df[\"currency_code\"].apply(usd_per_local)\ndf[\"rate_curr_per_usd\"] = df[\"rate_usd_per_curr\"].apply(\n    lambda r: None if r is None or r == 0 else round(1.0 / float(r), 9)\n)\n\n# QA: list missing currencies (no rate found)\nmissing_currencies = sorted(set(df.loc[df[\"rate_usd_per_curr\"].isna(), \"currency_code\"].tolist()))\nif missing_currencies:\n    print(\"⚠️ No FX rate configured for currencies:\", missing_currencies)\n\n# Keep only rows with a valid rate; drop dupes on key\ndf_nonnull = (\n    df[df[\"rate_usd_per_curr\"].notnull()]\n      .drop_duplicates(subset=[\"currency_code\", \"week_start_d\"])\n      .copy()\n)\n\nif df_nonnull.empty:\n    raise ValueError(\"All currencies missing FX rates. Populate FX_MAP or load a real FX source.\")\n\n# Persist to stage and COPY into the table with an explicit column list\ncsv_path = \"/tmp/fx_rates_weekly.csv\"\ndf_nonnull = df_nonnull[[\"currency_code\", \"week_start_d\", \"rate_usd_per_curr\", \"rate_curr_per_usd\"]]\ndf_nonnull.to_csv(csv_path, index=False)\n\n# Ensure stage exists (in case Cell 4_1 hasn't run yet)\nsession.sql(\"CREATE STAGE IF NOT EXISTS BCG.BCG_SCHEMA.BCG_STAGE\").collect()\n\nsession.file.put(csv_path, '@BCG.BCG_SCHEMA.BCG_STAGE', overwrite=True, auto_compress=False)\n\nsession.sql(\"\"\"\nCOPY INTO BCG.BCG_SCHEMA.T_FX_RATES_WEEKLY (currency_code, week_start_d, rate_usd_per_curr, rate_curr_per_usd)\nFROM @BCG.BCG_SCHEMA.BCG_STAGE\nFILES = ('fx_rates_weekly.csv')\nFILE_FORMAT = (TYPE = CSV, SKIP_HEADER = 1, FIELD_DELIMITER = ',')\nON_ERROR = 'ABORT_STATEMENT'\n\"\"\").collect()\n\n# Verify\nsession.sql(\"SELECT COUNT(*) AS rows_loaded FROM BCG.BCG_SCHEMA.T_FX_RATES_WEEKLY\").show()\nsession.sql(\"\"\"\nSELECT currency_code, week_start_d, rate_usd_per_curr, rate_curr_per_usd\nFROM BCG.BCG_SCHEMA.T_FX_RATES_WEEKLY\nORDER BY currency_code, week_start_d\nLIMIT 10\n\"\"\").show()\n\n# Optional: surface missing currencies to a Snowflake temp table for quick review\nif missing_currencies:\n    session.sql(\"CREATE OR REPLACE TEMP TABLE MISSING_FX_CURRENCIES (currency_code STRING)\").collect()\n    values_clause = \", \".join([f\"('{c}')\" for c in missing_currencies])\n    session.sql(f\"INSERT INTO MISSING_FX_CURRENCIES VALUES {values_clause}\").collect()\n    session.sql(\"SELECT * FROM MISSING_FX_CURRENCIES\").show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b3a0892-d24e-4265-b241-37206a60ebd8",
   "metadata": {
    "language": "sql",
    "name": "cell4_1",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\n-- Cell 4_1 — UPDATED\n\nUSE WAREHOUSE COMPUTE_WH;\nUSE DATABASE BCG;\nUSE SCHEMA BCG_SCHEMA;\n\n-- Ensure stage & CSV file format exist (used by Cell 3_1 fallback)\nCREATE STAGE IF NOT EXISTS BCG.BCG_SCHEMA.BCG_STAGE;\nCREATE FILE FORMAT IF NOT EXISTS BCG.BCG_SCHEMA.FF_CSV\n  TYPE = 'CSV'\n  SKIP_HEADER = 1\n  FIELD_DELIMITER = ','\n  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n  NULL_IF = ('', 'NULL');\n\n-- Landing table for raw ingestion\nCREATE OR REPLACE TABLE BCG.BCG_SCHEMA.T_BILLING_LANDING (\n  ProjectCode STRING,\n  ProjectName STRING,\n  ClientName STRING,\n  EmpID NUMBER(38,0),\n  EmpName STRING,\n\n  -- Store source dates as STRING (can be 'YYYY-MM-DD' or Excel serial text)\n  WeekStartDate STRING,\n  WeekEndDate   STRING,\n\n  CurrencyCode STRING,\n  HoursFilled NUMBER(18,2),\n  DailyHourRate NUMBER(18,2),\n  NetClientCharges NUMBER(18,2),\n  GrossClientCharges NUMBER(18,2),\n  PartnerEffortHours NUMBER(18,2),\n  PartnerCost NUMBER(18,2),\n  PartnerRevenue NUMBER(18,2),\n  TotalRevenue NUMBER(18,2),\n  Country STRING,\n  City STRING,\n  BillingStatus STRING,\n  InvoiceNumber STRING,\n  Department STRING,\n  Role STRING,\n\n  -- Project dates as STRING too\n  ProjectStartDate STRING,\n  ProjectEndDate   STRING,\n\n  \"Utilization%\" NUMBER(18,2),\n  \"DiscountApplied%\" NUMBER(18,2),\n  TaxAmount NUMBER(18,2),\n  BillingCycle STRING,\n  ProjectStatus STRING,\n  ClientIndustry STRING,\n  ManagerName STRING,\n  CostCenter STRING\n) COMMENT='Landing table: raw values; date columns kept as STRING for simple COPY.';\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc2719bc-d7ab-4981-a352-2726c0510177",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "\nUSE WAREHOUSE COMPUTE_WH;\nUSE DATABASE BCG;\nUSE SCHEMA BCG_SCHEMA;\n\n-- If the file is gzip, point to .csv.gz; adjust the file name if needed.\nCOPY INTO BCG.BCG_SCHEMA.T_BILLING_LANDING\nFROM @BCG.BCG_SCHEMA.BCG_STAGE/data_for_poc_project_data_1.csv.gz\nFILE_FORMAT = (TYPE = CSV, SKIP_HEADER = 1, FIELD_DELIMITER = ',', FIELD_OPTIONALLY_ENCLOSED_BY = '\"')\nON_ERROR = 'CONTINUE'   -- keep loading even if a few bad rows exist; switch to ABORT once clean\nFORCE = TRUE;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93d7b965-6c6d-4080-b5b4-026c209ed10c",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "\nUSE WAREHOUSE COMPUTE_WH;\nUSE DATABASE BCG;\nUSE SCHEMA BCG_SCHEMA;\n\nCREATE OR REPLACE VIEW BCG.BCG_SCHEMA.VW_BILLING_BASE\nCOMMENT = 'Curated base: robust DATE casting (ISO or serial) + weekly FX → USD duplicates.'\nAS\nWITH base AS (\n  SELECT\n    -- Canonical dims\n    TRIM(ProjectCode) AS project_code_d,\n    TRIM(ProjectName) AS project_name_d,\n    TRIM(ClientName)  AS client_name_d,\n    TRIM(Department)  AS department_d,\n    TRIM(Role)        AS role_d,\n    TRIM(Country)     AS country_d,\n    TRIM(City)        AS city_d,\n    TRIM(CurrencyCode) AS currency_code_d,\n    TRIM(InvoiceNumber) AS invoice_number_d,\n    TRIM(BillingStatus) AS billing_status_d,\n    TRIM(BillingCycle)  AS billing_cycle_d,\n    TRIM(ProjectStatus) AS project_status_d,\n    TRIM(ClientIndustry) AS client_industry_d,\n    TRIM(ManagerName)   AS manager_name_d,\n    TRIM(CostCenter)    AS cost_center_d,\n\n    /* Date casting: accept ISO 'YYYY-MM-DD' or Excel serial (as text) */\n    COALESCE(\n      TRY_TO_DATE(WeekStartDate),\n      DATEADD('day', TRY_TO_NUMBER(WeekStartDate), '1899-12-30')\n    ) AS week_start_d,\n\n    COALESCE(\n      TRY_TO_DATE(WeekEndDate),\n      DATEADD('day', TRY_TO_NUMBER(WeekEndDate), '1899-12-30')\n    ) AS week_end_d,\n\n    DATE_TRUNC(\n      'month',\n      COALESCE(TRY_TO_DATE(WeekStartDate),\n               DATEADD('day', TRY_TO_NUMBER(WeekStartDate), '1899-12-30'))\n    ) AS month_bucket_d,\n\n    COALESCE(\n      TRY_TO_DATE(ProjectStartDate),\n      DATEADD('day', TRY_TO_NUMBER(ProjectStartDate), '1899-12-30')\n    ) AS project_start_d,\n\n    COALESCE(\n      TRY_TO_DATE(ProjectEndDate),\n      DATEADD('day', TRY_TO_NUMBER(ProjectEndDate), '1899-12-30')\n    ) AS project_end_d,\n\n    -- Facts (explicit decimal types everywhere to avoid integer coercion)\n    TRY_TO_DECIMAL(HoursFilled,         18, 2) AS hours_f,\n    TRY_TO_DECIMAL(DailyHourRate,       18, 2) AS rate_f,\n    TRY_TO_DECIMAL(NetClientCharges,    18, 2) AS net_f,\n    TRY_TO_DECIMAL(GrossClientCharges,  18, 2) AS gross_f,\n    TRY_TO_DECIMAL(PartnerEffortHours,  18, 2) AS partner_hours_f,\n    TRY_TO_DECIMAL(PartnerCost,         18, 2) AS partner_cost_f,\n    TRY_TO_DECIMAL(PartnerRevenue,      18, 2) AS partner_rev_f,\n    TRY_TO_DECIMAL(TotalRevenue,        18, 2) AS total_rev_f,\n    TRY_TO_DECIMAL(\"Utilization%\",      18, 2)/100.0 AS utilization_f,\n    TRY_TO_DECIMAL(\"DiscountApplied%\",  18, 2)/100.0 AS discount_rate_f,\n    TRY_TO_DECIMAL(TaxAmount,           18, 2) AS tax_amount_f,\n\n    -- Validation flags (perform arithmetic in DECIMAL(18,2))\n    (ABS(\n       TRY_TO_DECIMAL(HoursFilled, 18, 2) * TRY_TO_DECIMAL(DailyHourRate, 18, 2)\n       - TRY_TO_DECIMAL(GrossClientCharges, 18, 2)\n     ) <= 0.01) AS gross_calc_ok_f,\n\n    (ABS(\n       TRY_TO_DECIMAL(PartnerEffortHours, 18, 2) * TRY_TO_DECIMAL(PartnerCost, 18, 2)\n       - TRY_TO_DECIMAL(PartnerRevenue, 18, 2)\n     ) <= 0.01) AS partner_calc_ok_f\n  FROM BCG.BCG_SCHEMA.T_BILLING_LANDING\n)\nSELECT\n  b.*,\n  fx.rate_usd_per_curr,\n\n  -- USD duplicates (weekly FX join; NVL for safety). All arithmetic in DECIMAL(18,2).\n  CASE WHEN b.currency_code_d = 'USD' THEN b.rate_f\n       ELSE b.rate_f * NVL(fx.rate_usd_per_curr, 1.0) END           AS rate_f_usd,\n\n  CASE WHEN b.currency_code_d = 'USD' THEN b.gross_f\n       ELSE b.gross_f * NVL(fx.rate_usd_per_curr, 1.0) END          AS gross_f_usd,\n\n  CASE WHEN b.currency_code_d = 'USD' THEN b.net_f\n       ELSE b.net_f * NVL(fx.rate_usd_per_curr, 1.0) END            AS net_f_usd,\n\n  CASE WHEN b.currency_code_d = 'USD' THEN b.partner_cost_f\n       ELSE b.partner_cost_f * NVL(fx.rate_usd_per_curr, 1.0) END   AS partner_cost_f_usd,\n\n  CASE WHEN b.currency_code_d = 'USD' THEN b.partner_rev_f\n       ELSE b.partner_rev_f * NVL(fx.rate_usd_per_curr, 1.0) END    AS partner_rev_f_usd,\n\n  CASE WHEN b.currency_code_d = 'USD' THEN b.total_rev_f\n       ELSE b.total_rev_f * NVL(fx.rate_usd_per_curr, 1.0) END      AS total_rev_f_usd,\n\n  CASE WHEN b.currency_code_d = 'USD' THEN b.tax_amount_f\n       ELSE b.tax_amount_f * NVL(fx.rate_usd_per_curr, 1.0) END     AS tax_amount_usd\n\nFROM base b\nLEFT JOIN BCG.BCG_SCHEMA.T_FX_RATES_WEEKLY fx\n  ON fx.currency_code = b.currency_code_d\n AND fx.week_start_d  = b.week_start_d;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16c7f871-6d36-43cf-9fa5-1be4b162cbaf",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "\nUSE WAREHOUSE COMPUTE_WH;\nUSE DATABASE BCG;\nUSE SCHEMA BCG_SCHEMA;\n\nCREATE OR REPLACE VIEW BCG.BCG_SCHEMA.VW_CLIENT_DIM AS\nSELECT DISTINCT\n  UPPER(TRIM(ClientName)) AS client_name_clean_d,\n  MD5(UPPER(TRIM(ClientName))) AS client_sk_d\nFROM BCG.BCG_SCHEMA.T_BILLING_LANDING\nWHERE ClientName IS NOT NULL;\n\nCREATE OR REPLACE VIEW BCG.BCG_SCHEMA.VW_PROJECT_DIM AS\nSELECT DISTINCT\n  UPPER(TRIM(ProjectCode)) AS project_code_clean_d,\n  UPPER(TRIM(ProjectName)) AS project_name_clean_d,\n  MD5(UPPER(TRIM(ProjectCode))) AS project_sk_d\nFROM BCG.BCG_SCHEMA.T_BILLING_LANDING\nWHERE ProjectCode IS NOT NULL;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6297196-a1cc-4337-8c65-df6e1e719175",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "\n-- CELL 8.1: Monthly × Currency aggregation MV (single-table; NO DISTINCT aggregates)\nCREATE OR REPLACE MATERIALIZED VIEW BCG.BCG_SCHEMA.MV_MONTH_CURRENCY_AGG\nCOMMENT = 'Monthly currency-level aggregates from T_BILLING_LANDING; compile-safe single-table MV without DISTINCT. Note: MVs consume credits.'\nCLUSTER BY (month_bucket_d, currency_code_d)\nAS\nSELECT\n  /* month bucket using the same casting rule used in VW_BILLING_BASE */\n  DATE_TRUNC('month',\n    COALESCE(\n      TRY_TO_DATE(t.WeekStartDate),\n      DATEADD('day', TRY_TO_NUMBER(t.WeekStartDate), '1899-12-30')\n    )\n  ) AS month_bucket_d,\n  t.CurrencyCode AS currency_code_d,\n\n  /* local aggregates with decimal safety */\n  SUM(TRY_TO_DECIMAL(t.GrossClientCharges, 18, 2)) AS total_gross_m,\n  SUM(TRY_TO_DECIMAL(t.NetClientCharges,   18, 2)) AS total_net_m,\n  AVG(TRY_TO_DECIMAL(t.DailyHourRate,      18, 2)) AS avg_rate_m,\n  SUM(TRY_TO_DECIMAL(t.HoursFilled,        18, 2)) AS hours_m\nFROM BCG.BCG_SCHEMA.T_BILLING_LANDING AS t\nGROUP BY\n  DATE_TRUNC('month',\n    COALESCE(\n      TRY_TO_DATE(t.WeekStartDate),\n      DATEADD('day', TRY_TO_NUMBER(t.WeekStartDate), '1899-12-30')\n    )\n  ),\n  t.CurrencyCode;\n\n\n\n-- CELL 8.2: Client × Month aggregation MV (single-table; already compliant)\nCREATE OR REPLACE MATERIALIZED VIEW BCG.BCG_SCHEMA.MV_CLIENT_MONTH_AGG\nCOMMENT = 'Client-month aggregates from T_BILLING_LANDING; compile-safe single-table MV. Note: MVs consume credits.'\nCLUSTER BY (month_bucket_d, client_name_d)\nAS\nSELECT\n  DATE_TRUNC('month',\n    COALESCE(\n      TRY_TO_DATE(t.WeekStartDate),\n      DATEADD('day', TRY_TO_NUMBER(t.WeekStartDate), '1899-12-30')\n    )\n  ) AS month_bucket_d,\n  t.ClientName AS client_name_d,\n\n  /* local aggregates with decimal safety */\n  SUM(TRY_TO_DECIMAL(t.TotalRevenue,      18, 2)) AS total_revenue_m,\n  SUM(TRY_TO_DECIMAL(t.NetClientCharges,  18, 2)) AS total_net_m,\n  AVG(TRY_TO_DECIMAL(t.DailyHourRate,     18, 2)) AS avg_rate_m,\n  SUM(TRY_TO_DECIMAL(t.HoursFilled,       18, 2)) AS hours_m\nFROM BCG.BCG_SCHEMA.T_BILLING_LANDING AS t\nGROUP BY\n  DATE_TRUNC('month',\n    COALESCE(\n      TRY_TO_DATE(t.WeekStartDate),\n      DATEADD('day', TRY_TO_NUMBER(t.WeekStartDate), '1899-12-30')\n    )\n  ),\n  t.ClientName;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5809e724-2491-4c84-b60d-84e846b2b83d",
   "metadata": {
    "language": "sql",
    "name": "cell9"
   },
   "outputs": [],
   "source": "\nCREATE OR REPLACE VIEW BCG.BCG_SCHEMA.V_BILLING_SEMANTIC AS\n/* \n  - FX: T_FX_RATES_WEEKLY(currency_code, week_start_d, rate_usd_per_curr, rate_curr_per_usd)\n        where rate_usd_per_curr = USD per 1 local unit.\n  - Landing: T_BILLING_LANDING with numeric percent columns:\n      \"Utilization%\" NUMBER(18,2), \"DiscountApplied%\" NUMBER(18,2)\n  - Row-level conversion first; aggregate in BI later.\n*/\n\nWITH l AS (\n  SELECT\n    -- Raw columns\n    ProjectCode, ProjectName, ClientName, EmpID, EmpName,\n    WeekStartDate, WeekEndDate,\n    CurrencyCode,\n    HoursFilled, DailyHourRate, NetClientCharges, GrossClientCharges,\n    PartnerEffortHours, PartnerCost, PartnerRevenue, TotalRevenue,\n    Country, City, BillingStatus, InvoiceNumber, Department, Role,\n    ProjectStartDate, ProjectEndDate, \"Utilization%\", \"DiscountApplied%\",\n    TaxAmount, BillingCycle, ProjectStatus, ClientIndustry, ManagerName, CostCenter,\n\n    -- Normalize week start date (ISO string or Excel serial)\n    COALESCE(\n      TRY_TO_DATE(WeekStartDate),\n      DATEADD('day', TRY_TO_NUMBER(WeekStartDate), '1899-12-30')\n    ) AS week_start_d_norm\n  FROM BCG.BCG_SCHEMA.T_BILLING_LANDING\n),\n\n-- De-duplicate FX to 1 row per (currency, week)\nfx_dedup AS (\n  SELECT\n    currency_code,\n    week_start_d,\n    rate_usd_per_curr,\n    rate_curr_per_usd\n  FROM BCG.BCG_SCHEMA.T_FX_RATES_WEEKLY\n  QUALIFY ROW_NUMBER() OVER (PARTITION BY currency_code, week_start_d ORDER BY currency_code) = 1\n),\n\n-- (currency, week) present in facts\nweeks AS (\n  SELECT DISTINCT\n    UPPER(TRIM(CurrencyCode)) AS currency_code,\n    week_start_d_norm         AS week_start_d\n  FROM l\n  WHERE week_start_d_norm IS NOT NULL\n    AND CurrencyCode IS NOT NULL\n),\n\n-- Forward-fill FX across weeks per currency\nfx_filled AS (\n  SELECT\n    w.currency_code,\n    w.week_start_d,\n    d.rate_usd_per_curr AS rate_usd_per_curr_exact,\n    d.rate_curr_per_usd AS rate_curr_per_usd_exact,\n\n    LAST_VALUE(d.rate_usd_per_curr) IGNORE NULLS OVER (\n      PARTITION BY w.currency_code\n      ORDER BY w.week_start_d\n      ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) AS rate_usd_per_curr_ff,\n\n    LAST_VALUE(d.rate_curr_per_usd) IGNORE NULLS OVER (\n      PARTITION BY w.currency_code\n      ORDER BY w.week_start_d\n      ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) AS rate_curr_per_usd_ff\n  FROM weeks w\n  LEFT JOIN fx_dedup d\n    ON d.currency_code = w.currency_code\n   AND d.week_start_d  = w.week_start_d\n),\n\nfx_best AS (\n  SELECT\n    currency_code,\n    week_start_d,\n    COALESCE(rate_usd_per_curr_exact, rate_usd_per_curr_ff) AS rate_usd_per_curr,\n    COALESCE(rate_curr_per_usd_exact, rate_curr_per_usd_ff) AS rate_curr_per_usd,\n    CASE \n      WHEN rate_usd_per_curr_exact IS NOT NULL THEN 'EXACT'\n      WHEN rate_usd_per_curr_ff    IS NOT NULL THEN 'FFILL'\n      ELSE 'MISSING'\n    END AS fx_match_type\n  FROM fx_filled\n),\n\n-- Percent normalization: input is numeric already (NUMBER(18,2))\nnorm AS (\n  SELECT\n    l.*,\n\n    /* Normalize to 0..100 domain\n       - treat <=1 as fraction (0..1) → *100\n       - treat 1..100 as percent already\n       - otherwise NULL (and flag later)\n    */\n    CASE\n      WHEN \"Utilization%\" IS NULL THEN NULL\n      WHEN \"Utilization%\" <= 1 THEN \"Utilization%\" * 100\n      WHEN \"Utilization%\" > 1 AND \"Utilization%\" <= 100 THEN \"Utilization%\"\n      ELSE NULL\n    END AS Utilization_Pct_100,\n\n    CASE\n      WHEN \"DiscountApplied%\" IS NULL THEN NULL\n      WHEN \"DiscountApplied%\" <= 1 THEN \"DiscountApplied%\" * 100\n      WHEN \"DiscountApplied%\" > 1 AND \"DiscountApplied%\" <= 100 THEN \"DiscountApplied%\"\n      ELSE NULL\n    END AS DiscountApplied_Pct_100\n  FROM l\n)\n\nSELECT\n  -- Dimensions\n  n.ProjectCode, n.ProjectName, n.ClientName, n.EmpID, n.EmpName,\n  n.Country, n.City, n.Department, n.Role, n.BillingStatus, n.InvoiceNumber,\n  n.BillingCycle, n.ProjectStatus, n.ClientIndustry, n.ManagerName, n.CostCenter,\n\n  -- Dates\n  n.WeekStartDate, n.WeekEndDate, n.week_start_d_norm AS WeekStartDate_D,\n  n.ProjectStartDate, n.ProjectEndDate,\n\n  -- Currency & FX\n  UPPER(TRIM(n.CurrencyCode)) AS CurrencyCode,\n  fx.rate_usd_per_curr,\n  fx.rate_curr_per_usd,\n  fx.fx_match_type,\n\n  -- Local measures\n  n.HoursFilled,\n  n.DailyHourRate,\n  n.NetClientCharges,\n  n.GrossClientCharges,\n  n.PartnerEffortHours,\n  n.PartnerCost,\n  n.PartnerRevenue,\n  n.TotalRevenue,\n\n  -- ✅ USD conversions (USD per local → multiply)\n  (n.DailyHourRate      * fx.rate_usd_per_curr) AS DailyHourRate_USD,\n  (n.NetClientCharges   * fx.rate_usd_per_curr) AS NetClientCharges_USD,\n  (n.GrossClientCharges * fx.rate_usd_per_curr) AS GrossClientCharges_USD,\n  (n.PartnerCost        * fx.rate_usd_per_curr) AS PartnerCost_USD,\n  (n.PartnerRevenue     * fx.rate_usd_per_curr) AS PartnerRevenue_USD,\n  (n.TotalRevenue       * fx.rate_usd_per_curr) AS TotalRevenue_USD,\n\n  -- ✅ Percents normalized (0..100) + helpers\n  n.Utilization_Pct_100,\n  n.DiscountApplied_Pct_100,\n  ROUND(n.Utilization_Pct_100, 2)       AS Utilization_Pct_100_Rounded,\n  ROUND(n.DiscountApplied_Pct_100, 2)   AS DiscountApplied_Pct_100_Rounded,\n\n  -- Weighted average helpers\n  (n.Utilization_Pct_100 * NULLIF(n.HoursFilled, 0))         AS Utilization_Pct_WNumer,\n  (n.DiscountApplied_Pct_100 * NULLIF(n.NetClientCharges, 0)) AS DiscountApplied_Pct_WNumer,\n\n  -- QA flags\n  (fx.rate_usd_per_curr IS NULL) AS FX_Missing_Flag,\n  CASE \n    WHEN fx.rate_usd_per_curr IS NULL THEN NULL\n    WHEN UPPER(TRIM(n.CurrencyCode)) = 'USD' AND ABS(fx.rate_usd_per_curr - 1.0) > 0.000001 THEN TRUE\n    ELSE FALSE\n  END AS FX_Unexpected_USD_Rate_Flag,\n\n  CASE\n    WHEN \"Utilization%\" IS NOT NULL AND (\"Utilization%\" < 0 OR \"Utilization%\" > 100) THEN TRUE\n    ELSE FALSE\n  END AS Utilization_OutOfRange_Flag,\n\n  -- Traceability\n  CASE \n    WHEN fx.rate_usd_per_curr IS NULL THEN NULL\n    ELSE 'USD_PER_LOCAL_MULTIPLY'\n  END AS FX_Method\n\nFROM norm n\nLEFT JOIN fx_best fx\n  ON fx.currency_code = UPPER(TRIM(n.CurrencyCode))\n AND fx.week_start_d  = n.week_start_d_norm;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "22112600-9eb4-4db4-86c2-e83e7e86b6c7",
   "metadata": {
    "name": "cell33",
    "collapsed": false
   },
   "source": "no need to execute the below\n"
  },
  {
   "cell_type": "code",
   "id": "db80b201-c06d-4d06-b377-8dc5367d335b",
   "metadata": {
    "language": "sql",
    "name": "cell34"
   },
   "outputs": [],
   "source": "\nCREATE OR REPLACE VIEW BCG.BCG_SCHEMA.VM_SEM_BILLING AS\n/* \n  - FX table: BCG.BCG_SCHEMA.T_FX_RATES_WEEKLY(currency_code, week_start_d, rate_usd_per_curr, rate_curr_per_usd)\n      where rate_usd_per_curr = USD per 1 local unit (e.g., INR → 0.012).\n  - Landing table: BCG.BCG_SCHEMA.T_BILLING_LANDING\n      with numeric percent cols: \"Utilization%\", \"DiscountApplied%\" (NUMBER(18,2)).\n  - Row-level conversion first; aggregate in BI later.\n*/\n\nWITH l AS (\n  SELECT\n    -- Raw columns\n    ProjectCode, ProjectName, ClientName, EmpID, EmpName,\n    WeekStartDate, WeekEndDate,\n    CurrencyCode,\n    HoursFilled, DailyHourRate, NetClientCharges, GrossClientCharges,\n    PartnerEffortHours, PartnerCost, PartnerRevenue, TotalRevenue,\n    Country, City, BillingStatus, InvoiceNumber, Department, Role,\n    ProjectStartDate, ProjectEndDate, \"Utilization%\", \"DiscountApplied%\",\n    TaxAmount, BillingCycle, ProjectStatus, ClientIndustry, ManagerName, CostCenter,\n\n    -- Normalize week-start date (supports ISO date or Excel serial)\n    COALESCE(\n      TRY_TO_DATE(WeekStartDate),\n      DATEADD('day', TRY_TO_NUMBER(WeekStartDate), '1899-12-30')\n    ) AS week_start_d_norm\n  FROM BCG.BCG_SCHEMA.T_BILLING_LANDING\n),\n\n-- Ensure one FX row per (currency, week)\nfx_dedup AS (\n  SELECT\n    currency_code,\n    week_start_d,\n    rate_usd_per_curr,\n    rate_curr_per_usd\n  FROM BCG.BCG_SCHEMA.T_FX_RATES_WEEKLY\n  QUALIFY ROW_NUMBER() OVER (PARTITION BY currency_code, week_start_d ORDER BY currency_code) = 1\n),\n\n-- (currency, week) pairs present in facts\nweeks AS (\n  SELECT DISTINCT\n    UPPER(TRIM(CurrencyCode)) AS currency_code,\n    week_start_d_norm         AS week_start_d\n  FROM l\n  WHERE week_start_d_norm IS NOT NULL\n    AND CurrencyCode IS NOT NULL\n),\n\n-- Forward-fill FX across weeks per currency\nfx_filled AS (\n  SELECT\n    w.currency_code,\n    w.week_start_d,\n    d.rate_usd_per_curr AS rate_usd_per_curr_exact,\n    d.rate_curr_per_usd AS rate_curr_per_usd_exact,\n\n    LAST_VALUE(d.rate_usd_per_curr) IGNORE NULLS OVER (\n      PARTITION BY w.currency_code\n      ORDER BY w.week_start_d\n      ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) AS rate_usd_per_curr_ff,\n\n    LAST_VALUE(d.rate_curr_per_usd) IGNORE NULLS OVER (\n      PARTITION BY w.currency_code\n      ORDER BY w.week_start_d\n      ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) AS rate_curr_per_usd_ff\n  FROM weeks w\n  LEFT JOIN fx_dedup d\n    ON d.currency_code = w.currency_code\n   AND d.week_start_d  = w.week_start_d\n),\n\nfx_best AS (\n  SELECT\n    currency_code,\n    week_start_d,\n    COALESCE(rate_usd_per_curr_exact, rate_usd_per_curr_ff) AS rate_usd_per_curr,\n    COALESCE(rate_curr_per_usd_exact, rate_curr_per_usd_ff) AS rate_curr_per_usd,\n    CASE \n      WHEN rate_usd_per_curr_exact IS NOT NULL THEN 'EXACT'\n      WHEN rate_usd_per_curr_ff    IS NOT NULL THEN 'FFILL'\n      ELSE 'MISSING'\n    END AS fx_match_type\n  FROM fx_filled\n),\n\n-- Percent normalization (input is numeric NUMBER(18,2)):\n-- 0..1 treated as fraction → *100; 1..100 treated as percent; else NULL\nnorm AS (\n  SELECT\n    l.*,\n    CASE\n      WHEN \"Utilization%\" IS NULL THEN NULL\n      WHEN \"Utilization%\" <= 1 THEN \"Utilization%\" * 100\n      WHEN \"Utilization%\" > 1 AND \"Utilization%\" <= 100 THEN \"Utilization%\"\n      ELSE NULL\n    END AS Utilization_Pct_100,\n\n    CASE\n      WHEN \"DiscountApplied%\" IS NULL THEN NULL\n      WHEN \"DiscountApplied%\" <= 1 THEN \"DiscountApplied%\" * 100\n      WHEN \"DiscountApplied%\" > 1 AND \"DiscountApplied%\" <= 100 THEN \"DiscountApplied%\"\n      ELSE NULL\n    END AS DiscountApplied_Pct_100\n  FROM l\n)\n\nSELECT\n  -- Dimensions\n  n.ProjectCode, n.ProjectName, n.ClientName, n.EmpID, n.EmpName,\n  n.Country, n.City, n.Department, n.Role, n.BillingStatus, n.InvoiceNumber,\n  n.BillingCycle, n.ProjectStatus, n.ClientIndustry, n.ManagerName, n.CostCenter,\n\n  -- Dates\n  n.WeekStartDate, n.WeekEndDate, n.week_start_d_norm AS WeekStartDate_D,\n  n.ProjectStartDate, n.ProjectEndDate,\n\n  -- Currency & FX context\n  UPPER(TRIM(n.CurrencyCode)) AS CurrencyCode,\n  fx.rate_usd_per_curr,\n  fx.rate_curr_per_usd,\n  fx.fx_match_type,\n\n  -- Local measures\n  n.HoursFilled,\n  n.DailyHourRate,\n  n.NetClientCharges,\n  n.GrossClientCharges,\n  n.PartnerEffortHours,\n  n.PartnerCost,\n  n.PartnerRevenue,\n  n.TotalRevenue,\n\n  -- ✅ USD conversions (USD per local → multiply)\n  (n.DailyHourRate      * fx.rate_usd_per_curr) AS DailyHourRate_USD,\n  (n.NetClientCharges   * fx.rate_usd_per_curr) AS NetClientCharges_USD,\n  (n.GrossClientCharges * fx.rate_usd_per_curr) AS GrossClientCharges_USD,\n  (n.PartnerCost        * fx.rate_usd_per_curr) AS PartnerCost_USD,\n  (n.PartnerRevenue     * fx.rate_usd_per_curr) AS PartnerRevenue_USD,\n  (n.TotalRevenue       * fx.rate_usd_per_curr) AS TotalRevenue_USD,\n\n  -- ✅ Percents normalized (0..100) + helpers\n  n.Utilization_Pct_100,\n  n.DiscountApplied_Pct_100,\n  ROUND(n.Utilization_Pct_100, 2)       AS Utilization_Pct_100_Rounded,\n  ROUND(n.DiscountApplied_Pct_100, 2)   AS DiscountApplied_Pct_100_Rounded,\n\n  -- Weighted-average helpers (build ratios in BI):\n  --   WAVG Utilization = SUM(Utilization_Pct_WNumer) / NULLIF(SUM(HoursFilled), 0)\n  (n.Utilization_Pct_100 * NULLIF(n.HoursFilled, 0))          AS Utilization_Pct_WNumer,\n  (n.DiscountApplied_Pct_100 * NULLIF(n.NetClientCharges, 0)) AS DiscountApplied_Pct_WNumer,\n\n  -- QA flags\n  (fx.rate_usd_per_curr IS NULL) AS FX_Missing_Flag,\n  CASE \n    WHEN fx.rate_usd_per_curr IS NULL THEN NULL\n    WHEN UPPER(TRIM(n.CurrencyCode)) = 'USD' AND ABS(fx.rate_usd_per_curr - 1.0) > 0.000001 THEN TRUE\n    ELSE FALSE\n  END AS FX_Unexpected_USD_Rate_Flag,\n\n  CASE\n    WHEN \"Utilization%\" IS NOT NULL AND (\"Utilization%\" < 0 OR \"Utilization%\" > 100) THEN TRUE\n    ELSE FALSE\n  END AS Utilization_OutOfRange_Flag,\n\n  -- Traceability\n  CASE \n    WHEN fx.rate_usd_per_curr IS NULL THEN NULL\n    ELSE 'USD_PER_LOCAL_MULTIPLY'\n  END AS FX_Method\n\nFROM norm n\nLEFT JOIN fx_best fx\n  ON fx.currency_code = UPPER(TRIM(n.CurrencyCode))\n AND fx.week_start_d  = n.week_start_d_norm;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "caa3fcad-1270-440e-8490-eb78522adb59",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": "\n-- CELL 11.1: Query history focused on SEMANTIC_VIEW / VW_SEM_BILLING usage\nCREATE OR REPLACE VIEW BCG.BCG_SCHEMA.VW_QH_ANALYST_SV\nCOMMENT = 'Query history filtered for SEMANTIC_VIEW operations and VW_SEM_BILLING usage.'\nAS\nSELECT\n  qh.QUERY_ID,\n  qh.USER_NAME,\n  qh.ROLE_NAME,\n  qh.WAREHOUSE_NAME,\n  qh.DATABASE_NAME,\n  qh.SCHEMA_NAME,\n  qh.QUERY_TEXT,\n  qh.START_TIME,\n  qh.END_TIME,\n  qh.EXECUTION_STATUS,\n  qh.BYTES_SCANNED,\n  qh.ROWS_PRODUCED\nFROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY AS qh\nWHERE qh.QUERY_TEXT ILIKE '%SEMANTIC_VIEW%'\n   OR qh.QUERY_TEXT ILIKE '%VW_SEM_BILLING%';\n\n-- CELL 11.2: Warehouse metering focused on PB_WH\nCREATE OR REPLACE VIEW BCG.BCG_SCHEMA.VW_WH_METERING_PB\nCOMMENT = 'Warehouse metering for PB_WH (credits, compute time) to monitor trial costs.'\nAS\nSELECT\n  wm.WAREHOUSE_NAME,\n  wm.START_TIME,\n  wm.END_TIME,\n  wm.CREDITS_USED,\n  wm.CREDITS_USED_COMPUTE,\n  wm.CREDITS_USED_CLOUD_SERVICES\nFROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY AS wm\nWHERE wm.WAREHOUSE_NAME = 'COMPUTE_WH';\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68cd6813-9c43-4321-99b3-c794ea42cff6",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE SEMANTIC VIEW BCG.BCG_SCHEMA.VW_SEM_BILLING\nTABLES (\n  po AS BCG.BCG_SCHEMA.VW_BILLING_BASE PRIMARY KEY (project_code_d)\n)\nFACTS (\n  -- local & USD facts exposed from the base view\n  PUBLIC po.hours_f                 AS po.hours_f             WITH SYNONYMS = ('hours', 'worked hours')         COMMENT = 'Hours filled for the employee-week',\n  PUBLIC po.rate_f                  AS po.rate_f              WITH SYNONYMS = ('hourly rate', 'rate')           COMMENT = 'Daily hour rate (local currency)',\n  PUBLIC po.gross_f                 AS po.gross_f             WITH SYNONYMS = ('gross charges', 'gross')        COMMENT = 'Gross client charges (local)',\n  PUBLIC po.net_f                   AS po.net_f               WITH SYNONYMS = ('net charges', 'net')            COMMENT = 'Net client charges (local)',\n  PUBLIC po.partner_hours_f         AS po.partner_hours_f     COMMENT = 'Partner effort hours (unitless)',\n  PUBLIC po.partner_cost_f          AS po.partner_cost_f      COMMENT = 'Partner cost (local)',\n  PUBLIC po.partner_rev_f           AS po.partner_rev_f       COMMENT = 'Partner revenue (local)',\n  PUBLIC po.total_rev_f             AS po.total_rev_f         WITH SYNONYMS = ('total revenue', 'revenue')      COMMENT = 'Total revenue (local)',\n  PUBLIC po.discount_rate_f         AS po.discount_rate_f     WITH SYNONYMS = ('discount %')                    COMMENT = 'Discount applied percentage (unitless)',\n  PUBLIC po.utilization_f           AS po.utilization_f       WITH SYNONYMS = ('utilization %')                 COMMENT = 'Utilization percentage (unitless)',\n  PUBLIC po.gross_calc_ok_f         AS po.gross_calc_ok_f     COMMENT = 'Flag: gross calculation OK (boolean-like)',\n  PUBLIC po.partner_calc_ok_f       AS po.partner_calc_ok_f   COMMENT = 'Flag: partner calculation OK (boolean-like)',\n\n  -- USD facts (already computed in base view)\n  PUBLIC po.rate_f_usd              AS po.rate_f_usd          WITH SYNONYMS = ('rate usd')                      COMMENT = 'Hourly/daily rate converted to USD',\n  PUBLIC po.gross_f_usd             AS po.gross_f_usd         WITH SYNONYMS = ('gross usd')                     COMMENT = 'Gross charges converted to USD',\n  PUBLIC po.net_f_usd               AS po.net_f_usd           WITH SYNONYMS = ('net usd')                       COMMENT = 'Net charges converted to USD',\n  PUBLIC po.partner_cost_f_usd      AS po.partner_cost_f_usd  COMMENT = 'Partner cost converted to USD',\n  PUBLIC po.partner_rev_f_usd       AS po.partner_rev_f_usd   COMMENT = 'Partner revenue converted to USD',\n  PUBLIC po.total_rev_f_usd         AS po.total_rev_f_usd     WITH SYNONYMS = ('revenue usd')                   COMMENT = 'Total revenue converted to USD',\n  PUBLIC po.tax_amount_usd          AS po.tax_amount_usd      WITH SYNONYMS = ('tax usd')                       COMMENT = 'Tax amount converted to USD'\n)\nDIMENSIONS (\n  -- dimensional attributes\n  PUBLIC po.month_bucket_d          AS po.month_bucket_d      WITH SYNONYMS = ('month', 'month bucket')         COMMENT = 'Month bucket based on week_start_d',\n  PUBLIC po.currency_code_d         AS po.currency_code_d     WITH SYNONYMS = ('currency')                      COMMENT = 'Currency code from source',\n  PUBLIC po.client_name_d           AS po.client_name_d       WITH SYNONYMS = ('client')                        COMMENT = 'Client name',\n  PUBLIC po.project_code_d          AS po.project_code_d      WITH SYNONYMS = ('project code')                  COMMENT = 'Project code (primary key in table alias)',\n  PUBLIC po.project_name_d          AS po.project_name_d      WITH SYNONYMS = ('project')                       COMMENT = 'Project name',\n  PUBLIC po.manager_name_d          AS po.manager_name_d      WITH SYNONYMS = ('manager')                       COMMENT = 'Manager name',\n  PUBLIC po.department_d            AS po.department_d        WITH SYNONYMS = ('dept')                          COMMENT = 'Department',\n  PUBLIC po.role_d                  AS po.role_d              WITH SYNONYMS = ('employee role')                 COMMENT = 'Role',\n  PUBLIC po.country_d               AS po.country_d           WITH SYNONYMS = ('country')                       COMMENT = 'Country',\n  PUBLIC po.city_d                  AS po.city_d              WITH SYNONYMS = ('city')                          COMMENT = 'City',\n  PUBLIC po.billing_status_d        AS po.billing_status_d    WITH SYNONYMS = ('billing status')                COMMENT = 'Billing status',\n  PUBLIC po.invoice_number_d        AS po.invoice_number_d    WITH SYNONYMS = ('invoice')                       COMMENT = 'Invoice number',\n  PUBLIC po.billing_cycle_d         AS po.billing_cycle_d     WITH SYNONYMS = ('cycle')                         COMMENT = 'Billing cycle',\n  PUBLIC po.project_status_d        AS po.project_status_d    WITH SYNONYMS = ('status')                        COMMENT = 'Project status',\n  PUBLIC po.client_industry_d       AS po.client_industry_d   WITH SYNONYMS = ('industry')                      COMMENT = 'Client industry',\n  PUBLIC po.cost_center_d           AS po.cost_center_d       WITH SYNONYMS = ('cost center')                   COMMENT = 'Cost center'\n)\nMETRICS (\n  -- Local metrics (DECIMAL scale enforced on aggregates without TRY_ because inputs are numeric)\n  PUBLIC po.total_gross_m           AS TO_DECIMAL(SUM(po.gross_f),        18, 2)  WITH SYNONYMS = ('gross total')   COMMENT = 'Sum of gross client charges (local)',\n  PUBLIC po.total_net_m             AS TO_DECIMAL(SUM(po.net_f),          18, 2)  WITH SYNONYMS = ('net total')     COMMENT = 'Sum of net client charges (local)',\n  PUBLIC po.total_partner_rev_m     AS TO_DECIMAL(SUM(po.partner_rev_f),  18, 2)  COMMENT = 'Sum of partner revenue (local)',\n  PUBLIC po.total_partner_cost_m    AS TO_DECIMAL(SUM(po.partner_cost_f), 18, 2)  COMMENT = 'Sum of partner cost (local)',\n  PUBLIC po.total_revenue_m         AS TO_DECIMAL(SUM(po.total_rev_f),    18, 2)  WITH SYNONYMS = ('revenue total') COMMENT = 'Sum of total revenue (local)',\n  PUBLIC po.hours_total_m           AS TO_DECIMAL(SUM(po.hours_f),        18, 2)  WITH SYNONYMS = ('hours total')   COMMENT = 'Total hours (unitless aggregated)',\n  PUBLIC po.partner_hours_total_m   AS TO_DECIMAL(SUM(po.partner_hours_f),18, 2)  COMMENT = 'Total partner hours (unitless aggregated)',\n  PUBLIC po.avg_rate_m              AS TO_DECIMAL(AVG(po.rate_f),         18, 2)  WITH SYNONYMS = ('avg rate')      COMMENT = 'Average rate (local)',\n  PUBLIC po.avg_utilization_m       AS TO_DECIMAL(AVG(po.utilization_f),  18, 2)  WITH SYNONYMS = ('avg util')      COMMENT = 'Average utilization % (unitless)',\n  PUBLIC po.avg_discount_m          AS TO_DECIMAL(AVG(po.discount_rate_f),18, 2)  WITH SYNONYMS = ('avg discount')  COMMENT = 'Average discount % (unitless)',\n  PUBLIC po.invoice_count_m         AS COUNT(DISTINCT po.invoice_number_d)         WITH SYNONYMS = ('invoices')      COMMENT = 'Distinct invoice count',\n\n  -- Derived local margins/ratios (safe division)\n  PUBLIC po.margin_local_m          AS TO_DECIMAL(SUM(po.total_rev_f) - SUM(po.partner_cost_f), 18, 2) COMMENT = 'Local margin = revenue - partner cost',\n  PUBLIC po.gross_to_net_ratio_m    AS TO_DECIMAL(CASE WHEN SUM(po.gross_f) = 0 THEN NULL ELSE SUM(po.net_f) / SUM(po.gross_f) END, 18, 6) COMMENT = 'Net/Gross ratio (local)',\n\n  -- USD metrics\n  PUBLIC po.total_gross_usd_m       AS TO_DECIMAL(SUM(po.gross_f_usd),       18, 2) WITH SYNONYMS = ('gross usd total')    COMMENT = 'Sum of gross (USD)',\n  PUBLIC po.total_net_usd_m         AS TO_DECIMAL(SUM(po.net_f_usd),         18, 2) WITH SYNONYMS = ('net usd total')      COMMENT = 'Sum of net (USD)',\n  PUBLIC po.total_revenue_usd_m     AS TO_DECIMAL(SUM(po.total_rev_f_usd),   18, 2) WITH SYNONYMS = ('revenue usd total')  COMMENT = 'Sum of revenue (USD)',\n  PUBLIC po.avg_rate_usd_m          AS TO_DECIMAL(AVG(po.rate_f_usd),        18, 2) WITH SYNONYMS = ('avg rate usd')       COMMENT = 'Average rate (USD)',\n  PUBLIC po.total_tax_usd_m         AS TO_DECIMAL(SUM(po.tax_amount_usd),    18, 2) WITH SYNONYMS = ('tax usd total')      COMMENT = 'Sum of tax (USD)',\n\n  -- Derived USD metrics\n  PUBLIC po.margin_usd_m            AS TO_DECIMAL(SUM(po.total_rev_f_usd) - SUM(po.partner_cost_f_usd), 18, 2) COMMENT = 'USD margin = revenue_usd - partner_cost_usd',\n  PUBLIC po.revenue_after_tax_usd_m AS TO_DECIMAL(SUM(po.total_rev_f_usd) - SUM(po.tax_amount_usd),     18, 2) COMMENT = 'USD revenue minus tax (simple calc)',\n  PUBLIC po.client_count_m          AS COUNT(DISTINCT po.client_name_d)                                      COMMENT = 'Distinct client count'\n)\nCOMMENT = 'Expanded semantic view over VW_BILLING_BASE with enriched facts, dimensions, and local/USD metrics for Snowflake Cortex Analyst.';\n",
   "execution_count": null
  }
 ]
}
